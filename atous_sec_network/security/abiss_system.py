"""
ABISS System - Adaptive Behaviour Intelligence Security System
Sistema de segurança inteligente com comportamento adaptativo usando Gemma 3N
"""
import time
import json
import logging
import hashlib
import threading
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, field
from collections import defaultdict, deque
import numpy as np
import requests

try:
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    logging.warning("Transformers não disponível - funcionalidade limitada")


@dataclass
class ThreatPattern:
    """Padrão de ameaça aprendido pelo sistema"""
    pattern_type: str
    indicators: List[str]
    severity: float
    frequency: float
    description: str = ""
    created_at: float = field(default_factory=time.time)
    pattern_id: str = field(default_factory=lambda: hashlib.md5(str(time.time()).encode()).hexdigest()[:8])
    
    def match(self, data: Dict[str, Any]) -> float:
        """
        Calcula score de correspondência com dados
        
        Args:
            data: Dados para comparação
            
        Returns:
            Score de correspondência (0-1)
        """
        match_count = 0
        total_indicators = len(self.indicators)
        
        for indicator in self.indicators:
            if indicator in data or any(indicator in str(v) for v in data.values()):
                match_count += 1
        
        return match_count / total_indicators if total_indicators > 0 else 0.0


@dataclass
class AdaptiveResponse:
    """Resposta adaptativa gerada pelo sistema"""
    action: str
    priority: int
    parameters: Dict[str, Any]
    timestamp: float = field(default_factory=time.time)
    response_id: str = field(default_factory=lambda: hashlib.md5(str(time.time()).encode()).hexdigest()[:8])
    
    def execute(self) -> Dict[str, Any]:
        """
        Executa a resposta adaptativa
        
        Returns:
            Resultado da execução
        """
        start_time = time.time()
        
        try:
            # Implementação básica - em produção integrar com sistemas reais
            if self.action == "block_ip":
                # Simular bloqueio de IP
                ip = self.parameters.get("ip", "")
                duration = self.parameters.get("duration", 3600)
                success = True  # Simulação
                
            elif self.action == "rate_limit":
                # Simular rate limiting
                rate = self.parameters.get("rate", 100)
                window = self.parameters.get("window", 60)
                success = True  # Simulação
                
            elif self.action == "alert_admin":
                # Simular alerta para administrador
                message = self.parameters.get("message", "Security alert")
                success = True  # Simulação
                
            else:
                success = False
            
            execution_time = time.time() - start_time
            
            return {
                "success": success,
                "execution_time": execution_time,
                "action": self.action,
                "parameters": self.parameters
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "execution_time": time.time() - start_time
            }


class ABISSSystem:
    """
    Sistema ABISS - Adaptive Behaviour Intelligence Security System
    
    Sistema de segurança inteligente que:
    - Detecta ameaças usando IA (Gemma 3N)
    - Analisa comportamento de usuários
    - Gera respostas adaptativas
    - Aprende continuamente com novas ameaças
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        Inicializa o sistema ABISS
        
        Args:
            config: Configuração do sistema
        """
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # Modelo Gemma 3N
        self.model = None
        self.tokenizer = None
        self.pipeline = None
        self.model_name = config.get("model_name", "google/gemma-3n-2b")
        
        # Estruturas de dados
        self.threat_patterns = {}
        self.adaptive_responses = {}
        self.learning_history = deque(maxlen=config.get("memory_size", 1000))
        
        # Monitoramento em tempo real
        self.is_monitoring = False
        self.monitor_thread = None
        self.stop_monitoring = threading.Event()
        
        # Métricas e estatísticas
        self.threat_stats = defaultdict(int)
        self.response_stats = defaultdict(int)
        self.false_positive_rate = 0.0
        
        # Inicializar modelo
        self._initialize_model()
        
        # Carregar padrões conhecidos
        self._load_known_patterns()
        
        self.logger.info("Sistema ABISS inicializado com modelo Gemma 3N")
    
    def _initialize_model(self) -> None:
        """Inicializa o modelo Gemma 3N"""
        if not TRANSFORMERS_AVAILABLE:
            self.logger.warning("Transformers não disponível - usando modo simulação")
            return
        
        try:
            # Carregar tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            
            # Carregar modelo
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=torch.float16,
                device_map="auto"
            )
            
            # Configurar pipeline
            self.pipeline = pipeline(
                "text-generation",
                model=self.model,
                tokenizer=self.tokenizer,
                max_length=512,
                temperature=0.7
            )
            
            self.logger.info(f"Modelo Gemma 3N carregado: {self.model_name}")
            
        except Exception as e:
            self.logger.error(f"Falha ao carregar modelo Gemma 3N: {e}")
            self.model = None
            self.tokenizer = None
    
    def _load_known_patterns(self) -> None:
        """Carrega padrões de ameaça conhecidos"""
        known_patterns = [
            {
                "pattern_type": "brute_force",
                "indicators": ["multiple_failed_logins", "rapid_connection_attempts"],
                "severity": 0.8,
                "frequency": 0.1,
                "description": "Multiple failed login attempts from same source"
            },
            {
                "pattern_type": "ddos_attack",
                "indicators": ["high_packet_rate", "multiple_sources", "syn_flood"],
                "severity": 0.9,
                "frequency": 0.05,
                "description": "Distributed denial of service attack"
            },
            {
                "pattern_type": "data_exfiltration",
                "indicators": ["large_data_transfer", "unusual_destination", "encrypted_traffic"],
                "severity": 0.9,
                "frequency": 0.02,
                "description": "Suspicious data transfer patterns"
            },
            {
                "pattern_type": "malware_behavior",
                "indicators": ["file_creation", "registry_modification", "network_connection"],
                "severity": 0.8,
                "frequency": 0.03,
                "description": "Typical malware behavior pattern"
            }
        ]
        
        for pattern_data in known_patterns:
            pattern = ThreatPattern(**pattern_data)
            self.threat_patterns[pattern.pattern_id] = pattern
    
    def detect_threat(self, network_data: Dict[str, Any]) -> Tuple[float, str]:
        """
        Detecta ameaças usando IA e análise de padrões
        
        Args:
            network_data: Dados de rede para análise
            
        Returns:
            Tuple (score_ameaça, tipo_ameaça)
        """
        try:
            # Análise baseada em padrões
            pattern_scores = []
            for pattern in self.threat_patterns.values():
                match_score = pattern.match(network_data)
                if match_score > 0.5:  # Threshold para considerar correspondência
                    pattern_scores.append((match_score * pattern.severity, pattern.pattern_type))
            
            # Análise com IA (Gemma 3N)
            ai_score, ai_type = self._analyze_with_ai(network_data)
            
            # Combinar resultados
            if pattern_scores:
                best_pattern_score, best_pattern_type = max(pattern_scores, key=lambda x: x[0])
                combined_score = (best_pattern_score + ai_score) / 2
                combined_type = best_pattern_type if best_pattern_score > ai_score else ai_type
            else:
                combined_score = ai_score
                combined_type = ai_type
            
            # Atualizar estatísticas
            self.threat_stats[combined_type] += 1
            
            return combined_score, combined_type
            
        except Exception as e:
            self.logger.error(f"Erro na detecção de ameaças: {e}")
            return 0.0, "unknown"
    
    def _analyze_with_ai(self, network_data: Dict[str, Any]) -> Tuple[float, str]:
        """
        Analisa dados usando modelo Gemma 3N
        
        Args:
            network_data: Dados para análise
            
        Returns:
            Tuple (score, tipo_ameaça)
        """
        if self.pipeline is None:
            # Modo simulação
            return np.random.uniform(0.0, 1.0), "simulated_threat"
        
        try:
            # Preparar prompt para o modelo
            prompt = self._build_security_prompt(network_data)
            
            # Executar inferência
            response = self.pipeline(prompt, max_length=200, num_return_sequences=1)
            
            # Analisar resposta
            ai_response = response[0]["generated_text"]
            
            # Extrair score e tipo da resposta
            score, threat_type = self._parse_ai_response(ai_response)
            
            return score, threat_type
            
        except Exception as e:
            self.logger.error(f"Erro na análise com IA: {e}")
            return 0.0, "ai_error"
    
    def _build_security_prompt(self, network_data: Dict[str, Any]) -> str:
        """
        Constrói prompt para análise de segurança
        
        Args:
            network_data: Dados de rede
            
        Returns:
            Prompt estruturado
        """
        prompt = f"""
        Analise os seguintes dados de rede para detectar ameaças de segurança:
        
        Dados de Rede:
        - Pacotes: {network_data.get('packet_count', 0)}
        - Tentativas de conexão: {network_data.get('connection_attempts', 0)}
        - Taxa de transferência: {network_data.get('data_transfer_rate', 0)}
        - IPs de origem: {network_data.get('source_ips', [])}
        - Portas de destino: {network_data.get('destination_ports', [])}
        
        Avalie se há ameaças de segurança e responda no formato:
        THREAT_SCORE: [0.0-1.0]
        THREAT_TYPE: [tipo_da_ameaça]
        CONFIDENCE: [0.0-1.0]
        """
        
        return prompt
    
    def _parse_ai_response(self, response: str) -> Tuple[float, str]:
        """
        Analisa resposta do modelo IA
        
        Args:
            response: Resposta do modelo
            
        Returns:
            Tuple (score, tipo_ameaça)
        """
        try:
            lines = response.split('\n')
            score = 0.0
            threat_type = "unknown"
            
            for line in lines:
                if line.startswith("THREAT_SCORE:"):
                    score = float(line.split(":")[1].strip())
                elif line.startswith("THREAT_TYPE:"):
                    threat_type = line.split(":")[1].strip()
            
            return score, threat_type
            
        except Exception as e:
            self.logger.error(f"Erro ao analisar resposta da IA: {e}")
            return 0.0, "parse_error"
    
    def analyze_behavior(self, user_behavior: Dict[str, Any]) -> Tuple[float, List[Dict[str, Any]]]:
        """
        Analisa comportamento do usuário
        
        Args:
            user_behavior: Dados de comportamento
            
        Returns:
            Tuple (score_comportamento, anomalias_detectadas)
        """
        try:
            # Análise temporal
            time_score = self._analyze_temporal_patterns(user_behavior)
            
            # Análise de padrões de acesso
            access_score = self._analyze_access_patterns(user_behavior)
            
            # Análise de uso de rede
            network_score = self._analyze_network_usage(user_behavior)
            
            # Score combinado
            behavior_score = (time_score + access_score + network_score) / 3
            
            # Detectar anomalias
            anomalies = self._detect_behavior_anomalies(user_behavior)
            
            return behavior_score, anomalies
            
        except Exception as e:
            self.logger.error(f"Erro na análise comportamental: {e}")
            return 0.0, []
    
    def _analyze_temporal_patterns(self, behavior: Dict[str, Any]) -> float:
        """Analisa padrões temporais"""
        # Implementação básica
        login_time = behavior.get("login_time", "09:00")
        logout_time = behavior.get("logout_time", "17:00")
        
        # Verificar horários normais de trabalho
        if "09:00" <= login_time <= "10:00" and "16:00" <= logout_time <= "18:00":
            return 0.9
        elif "08:00" <= login_time <= "11:00" and "15:00" <= logout_time <= "19:00":
            return 0.7
        else:
            return 0.3
    
    def _analyze_access_patterns(self, behavior: Dict[str, Any]) -> float:
        """Analisa padrões de acesso"""
        access_pattern = behavior.get("data_access_pattern", [])
        
        # Verificar se acessa arquivos típicos
        typical_files = ["file1", "file2", "file3", "document", "report"]
        typical_count = sum(1 for file in access_pattern if any(tf in file.lower() for tf in typical_files))
        
        return min(1.0, typical_count / len(access_pattern)) if access_pattern else 0.5
    
    def _analyze_network_usage(self, behavior: Dict[str, Any]) -> float:
        """Analisa uso de rede"""
        network_usage = behavior.get("network_usage", 0)
        
        # Verificar se está dentro de limites normais (5MB - 50MB)
        if 5000000 <= network_usage <= 50000000:
            return 0.9
        elif 1000000 <= network_usage <= 100000000:
            return 0.7
        else:
            return 0.3
    
    def _detect_behavior_anomalies(self, behavior: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Detecta anomalias comportamentais"""
        anomalies = []
        
        # Verificar horário de login anômalo
        login_time = behavior.get("login_time", "")
        if login_time and ("02:00" <= login_time <= "06:00"):
            anomalies.append({
                "type": "anomalous_login_time",
                "severity": 0.7,
                "description": f"Login at unusual time: {login_time}"
            })
        
        # Verificar uso excessivo de rede
        network_usage = behavior.get("network_usage", 0)
        if network_usage > 100000000:  # 100MB
            anomalies.append({
                "type": "excessive_network_usage",
                "severity": 0.8,
                "description": f"Excessive network usage: {network_usage} bytes"
            })
        
        return anomalies
    
    def generate_adaptive_response(self, threat_data: Dict[str, Any]) -> AdaptiveResponse:
        """
        Gera resposta adaptativa para ameaça
        
        Args:
            threat_data: Dados da ameaça detectada
            
        Returns:
            Resposta adaptativa
        """
        threat_score = threat_data.get("threat_score", 0.0)
        threat_type = threat_data.get("threat_type", "unknown")
        source_ip = threat_data.get("source_ip", "")
        
        # Determinar ação baseada no tipo e score da ameaça
        if threat_score > 0.9:
            action = "block_ip"
            priority = 1
            parameters = {"ip": source_ip, "duration": 86400}  # 24 horas
            
        elif threat_score > 0.7:
            action = "rate_limit"
            priority = 2
            parameters = {"ip": source_ip, "rate": 10, "window": 60}
            
        elif threat_score > 0.5:
            action = "alert_admin"
            priority = 3
            parameters = {"message": f"Potential threat detected: {threat_type}"}
            
        else:
            action = "monitor"
            priority = 4
            parameters = {"ip": source_ip, "duration": 3600}
        
        response = AdaptiveResponse(
            action=action,
            priority=priority,
            parameters=parameters
        )
        
        # Armazenar resposta
        self.adaptive_responses[response.response_id] = response
        
        return response
    
    def learn_threat_pattern(self, pattern_data: Dict[str, Any]) -> str:
        """
        Aprende novo padrão de ameaça
        
        Args:
            pattern_data: Dados do novo padrão
            
        Returns:
            ID do padrão aprendido
        """
        pattern = ThreatPattern(**pattern_data)
        self.threat_patterns[pattern.pattern_id] = pattern
        
        self.logger.info(f"Novo padrão aprendido: {pattern.pattern_type}")
        return pattern.pattern_id
    
    def get_threat_pattern(self, pattern_id: str) -> Optional[ThreatPattern]:
        """Recupera padrão de ameaça por ID"""
        return self.threat_patterns.get(pattern_id)
    
    def evaluate_response_effectiveness(self, response: AdaptiveResponse, outcome: Dict[str, Any]) -> float:
        """
        Avalia eficácia de uma resposta
        
        Args:
            response: Resposta aplicada
            outcome: Resultado da resposta
            
        Returns:
            Score de eficácia (0-1)
        """
        threat_stopped = outcome.get("threat_stopped", False)
        false_positive = outcome.get("false_positive", False)
        response_time = outcome.get("response_time", 0.0)
        collateral_damage = outcome.get("collateral_damage", 0.0)
        
        # Calcular eficácia
        effectiveness = 0.0
        
        if threat_stopped and not false_positive:
            effectiveness += 0.6
        elif threat_stopped:
            effectiveness += 0.4
        elif false_positive:
            effectiveness -= 0.3
        
        # Considerar tempo de resposta
        if response_time < 1.0:
            effectiveness += 0.2
        elif response_time < 5.0:
            effectiveness += 0.1
        
        # Considerar dano colateral
        effectiveness -= collateral_damage
        
        return max(0.0, min(1.0, effectiveness))
    
    def learn_from_outcome(self, response: AdaptiveResponse, outcome: Dict[str, Any]) -> None:
        """
        Aprende com resultado de uma resposta
        
        Args:
            response: Resposta aplicada
            outcome: Resultado obtido
        """
        effectiveness = self.evaluate_response_effectiveness(response, outcome)
        
        # Registrar aprendizado
        learning_entry = {
            "response_id": response.response_id,
            "action": response.action,
            "effectiveness": effectiveness,
            "outcome": outcome,
            "timestamp": time.time()
        }
        
        self.learning_history.append(learning_entry)
        
        # Atualizar estatísticas
        self.response_stats[response.action] += 1
        
        # Ajustar thresholds se necessário
        if effectiveness < 0.3:
            self._adjust_thresholds_for_poor_performance()
    
    def _adjust_thresholds_for_poor_performance(self) -> None:
        """Ajusta thresholds para melhorar performance"""
        current_threshold = self.config["threat_threshold"]
        
        # Aumentar threshold se muitas respostas ineficazes
        if len(self.learning_history) > 10:
            recent_effectiveness = np.mean([
                entry["effectiveness"] 
                for entry in list(self.learning_history)[-10:]
            ])
            
            if recent_effectiveness < 0.5:
                self.config["threat_threshold"] = min(0.9, current_threshold + 0.05)
                self.logger.info(f"Threshold ajustado para: {self.config['threat_threshold']}")
    
    def share_threat_intelligence(self, threat_info: Dict[str, Any]) -> Dict[str, Any]:
        """
        Compartilha inteligência de ameaças (anônima)
        
        Args:
            threat_info: Informações da ameaça
            
        Returns:
            Dados anonimizados para compartilhamento
        """
        shared_data = {
            "threat_type": threat_info.get("threat_type", "unknown"),
            "indicators": threat_info.get("indicators", []),
            "severity": threat_info.get("severity", 0.0),
            "timestamp": time.time(),
            "anonymized": True,
            "region": self.config.get("region", "unknown")
        }
        
        return shared_data
    
    def establish_behavioral_baseline(self, historical_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Estabelece linha base comportamental
        
        Args:
            historical_data: Dados históricos de comportamento
            
        Returns:
            Linha base comportamental
        """
        if not historical_data:
            return {}
        
        # Analisar padrões de login
        login_times = [data.get("login_time", "09:00") for data in historical_data]
        avg_login_time = np.mean([self._time_to_minutes(lt) for lt in login_times])
        
        # Analisar padrões de acesso
        access_counts = [data.get("data_access_count", 0) for data in historical_data]
        avg_access_count = np.mean(access_counts)
        std_access_count = np.std(access_counts)
        
        # Analisar uso de rede
        network_usages = [data.get("network_usage", 0) for data in historical_data]
        avg_network_usage = np.mean(network_usages)
        std_network_usage = np.std(network_usages)
        
        baseline = {
            "login_patterns": {
                "avg_time": self._minutes_to_time(avg_login_time),
                "std_dev": 1.0
            },
            "data_access_patterns": {
                "avg_count": avg_access_count,
                "std_dev": std_access_count
            },
            "network_usage_patterns": {
                "avg_usage": avg_network_usage,
                "std_dev": std_network_usage
            }
        }
        
        return baseline
    
    def _time_to_minutes(self, time_str: str) -> float:
        """Converte string de tempo para minutos"""
        try:
            hours, minutes = map(int, time_str.split(":"))
            return hours * 60 + minutes
        except:
            return 540  # 9:00 como padrão
    
    def _minutes_to_time(self, minutes: float) -> str:
        """Converte minutos para string de tempo"""
        hours = int(minutes // 60)
        mins = int(minutes % 60)
        return f"{hours:02d}:{mins:02d}"
    
    def detect_anomalies(self, behavior: Dict[str, Any], baseline: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Detecta anomalias baseado na linha base
        
        Args:
            behavior: Comportamento atual
            baseline: Linha base comportamental
            
        Returns:
            Lista de anomalias detectadas
        """
        anomalies = []
        
        # Verificar anomalias de login
        if "login_patterns" in baseline:
            current_login = behavior.get("login_time", "09:00")
            avg_login = baseline["login_patterns"]["avg_time"]
            
            current_minutes = self._time_to_minutes(current_login)
            avg_minutes = self._time_to_minutes(avg_login)
            
            if abs(current_minutes - avg_minutes) > 120:  # 2 horas de diferença
                anomalies.append({
                    "type": "anomalous_login_time",
                    "severity": 0.7,
                    "description": f"Login time {current_login} differs significantly from baseline {avg_login}"
                })
        
        # Verificar anomalias de acesso
        if "data_access_patterns" in baseline:
            current_access = behavior.get("data_access_count", 0)
            avg_access = baseline["data_access_patterns"]["avg_count"]
            std_access = baseline["data_access_patterns"]["std_dev"]
            
            if abs(current_access - avg_access) > 2 * std_access:
                anomalies.append({
                    "type": "anomalous_data_access",
                    "severity": 0.8,
                    "description": f"Data access count {current_access} is significantly different from baseline {avg_access}"
                })
        
        return anomalies
    
    def optimize_responses(self, response_history: List[Tuple[AdaptiveResponse, Dict[str, Any]]]) -> Dict[str, Any]:
        """
        Otimiza respostas baseado em histórico
        
        Args:
            response_history: Histórico de respostas e resultados
            
        Returns:
            Otimizações sugeridas
        """
        if not response_history:
            return {}
        
        # Analisar eficácia por ação
        action_effectiveness = defaultdict(list)
        for response, outcome in response_history:
            effectiveness = self.evaluate_response_effectiveness(response, outcome)
            action_effectiveness[response.action].append(effectiveness)
        
        # Encontrar melhores ações
        best_actions = {}
        for action, effectiveness_list in action_effectiveness.items():
            avg_effectiveness = np.mean(effectiveness_list)
            best_actions[action] = avg_effectiveness
        
        # Otimizar parâmetros
        parameter_optimizations = {}
        for action in best_actions:
            if action == "block_ip":
                # Otimizar duração do bloqueio
                durations = [r.parameters.get("duration", 3600) for r, _ in response_history if r.action == action]
                if durations:
                    optimal_duration = np.median(durations)
                    parameter_optimizations[action] = {"optimal_duration": optimal_duration}
        
        return {
            "best_actions": best_actions,
            "parameter_optimizations": parameter_optimizations
        }
    
    def get_model_info(self) -> Dict[str, Any]:
        """Retorna informações sobre o modelo"""
        return {
            "model_name": self.model_name,
            "model_loaded": self.model is not None,
            "model_size": "2B" if "2b" in self.model_name else "Unknown",
            "transformers_available": TRANSFORMERS_AVAILABLE
        }
    
    def run_model_inference(self, input_text: str) -> Dict[str, Any]:
        """
        Executa inferência no modelo Gemma 3N
        
        Args:
            input_text: Texto de entrada
            
        Returns:
            Resultado da inferência
        """
        if self.pipeline is None:
            return {
                "analysis": "Model not available",
                "confidence": 0.0,
                "error": "Model not loaded"
            }
        
        try:
            response = self.pipeline(input_text, max_length=200, num_return_sequences=1)
            result_text = response[0]["generated_text"]
            
            return {
                "analysis": result_text,
                "confidence": 0.8,
                "model": self.model_name
            }
            
        except Exception as e:
            return {
                "analysis": "Error in inference",
                "confidence": 0.0,
                "error": str(e)
            }
    
    def start_real_time_monitoring(self) -> None:
        """Inicia monitoramento em tempo real"""
        if self.is_monitoring:
            return
        
        self.is_monitoring = True
        self.stop_monitoring.clear()
        
        self.monitor_thread = threading.Thread(target=self._monitoring_loop)
        self.monitor_thread.daemon = True
        self.monitor_thread.start()
        
        self.logger.info("Monitoramento em tempo real iniciado")
    
    def stop_real_time_monitoring(self) -> None:
        """Para monitoramento em tempo real"""
        self.is_monitoring = False
        self.stop_monitoring.set()
        
        if self.monitor_thread:
            self.monitor_thread.join()
        
        self.logger.info("Monitoramento em tempo real parado")
    
    def _monitoring_loop(self) -> None:
        """Loop de monitoramento em tempo real"""
        while not self.stop_monitoring.is_set():
            try:
                # Aqui seria integrado com coleta real de dados
                time.sleep(1)  # Intervalo de verificação
                
            except Exception as e:
                self.logger.error(f"Erro no loop de monitoramento: {e}")
                time.sleep(5)
    
    def process_real_time_data(self, data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Processa dados em tempo real
        
        Args:
            data: Dados em tempo real
            
        Returns:
            Lista de alertas gerados
        """
        alerts = []
        
        # Detectar ameaças
        threat_score, threat_type = self.detect_threat(data)
        
        if threat_score > self.config["threat_threshold"]:
            alerts.append({
                "type": "threat_detected",
                "severity": threat_score,
                "description": f"Threat detected: {threat_type}",
                "timestamp": time.time()
            })
        
        return alerts
    
    def correlate_threats(self, threats: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Correlaciona múltiplas ameaças
        
        Args:
            threats: Lista de ameaças
            
        Returns:
            Análise de correlação
        """
        if len(threats) < 2:
            return {"campaign_detected": False}
        
        # Agrupar por IP de origem
        ip_groups = defaultdict(list)
        for threat in threats:
            source_ip = threat.get("source_ip", "unknown")
            ip_groups[source_ip].append(threat)
        
        # Verificar campanhas
        campaigns = []
        for ip, ip_threats in ip_groups.items():
            if len(ip_threats) >= 2:
                # Verificar sequência temporal
                sorted_threats = sorted(ip_threats, key=lambda x: x.get("timestamp", 0))
                
                campaign = {
                    "source_ip": ip,
                    "threat_count": len(ip_threats),
                    "time_span": sorted_threats[-1]["timestamp"] - sorted_threats[0]["timestamp"],
                    "threat_types": [t.get("type", "unknown") for t in ip_threats],
                    "max_severity": max(t.get("severity", 0) for t in ip_threats)
                }
                campaigns.append(campaign)
        
        return {
            "campaign_detected": len(campaigns) > 0,
            "campaigns": campaigns,
            "threat_chain": self._identify_threat_chain(threats),
            "overall_severity": max(t.get("severity", 0) for t in threats)
        }
    
    def _identify_threat_chain(self, threats: List[Dict[str, Any]]) -> List[str]:
        """Identifica cadeia de ameaças"""
        # Implementação básica - em produção usar análise mais sofisticada
        threat_types = [t.get("type", "unknown") for t in threats]
        
        # Padrões conhecidos de cadeias de ameaças
        known_chains = [
            ["port_scan", "brute_force", "data_exfiltration"],
            ["phishing", "malware_infection", "data_exfiltration"],
            ["ddos_attack", "data_exfiltration"]
        ]
        
        for chain in known_chains:
            if all(tt in threat_types for tt in chain):
                return chain
        
        return threat_types
    
    def adjust_thresholds(self, environmental_factors: Dict[str, Any]) -> None:
        """
        Ajusta thresholds baseado em fatores ambientais
        
        Args:
            environmental_factors: Fatores ambientais
        """
        network_load = environmental_factors.get("network_load", 0.5)
        threat_landscape = environmental_factors.get("threat_landscape", "medium")
        false_positive_rate = environmental_factors.get("false_positive_rate", 0.1)
        
        # Ajustar threshold baseado na paisagem de ameaças
        if threat_landscape == "high":
            self.config["threat_threshold"] = max(0.5, self.config["threat_threshold"] - 0.1)
        elif threat_landscape == "low":
            self.config["threat_threshold"] = min(0.9, self.config["threat_threshold"] + 0.1)
        
        # Ajustar baseado na taxa de falsos positivos
        if false_positive_rate > 0.15:
            self.config["threat_threshold"] = min(0.9, self.config["threat_threshold"] + 0.05)
        elif false_positive_rate < 0.05:
            self.config["threat_threshold"] = max(0.5, self.config["threat_threshold"] - 0.05)
        
        self.logger.info(f"Thresholds ajustados para: {self.config['threat_threshold']}")